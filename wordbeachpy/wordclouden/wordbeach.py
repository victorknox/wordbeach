# -*- coding: utf-8 -*-
"""wordcloud

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1km5DCNMDaltaGOJDKlRMuN6SeKr5Pxhv
"""

# pip install requests
"""## Counting the number of sentences

"""

# Opening a file
file = open("text.txt","r")
Counter = 0
  
# Reading from file
Content = file.read()
CoList = Content.split(".")
  
for i in CoList:
    if i:
        Counter += 1
          
print("This is the number of lines in the file")
print(Counter)

"""## Cleaning the text

"""
file = open("text.txt","r")
Counter = 0
  
# cleaning text
import re
# x = re.sub("[^ A-Za-z]", "", x)

x = Content.lower()
# Remove unicode characters
x = x.encode('ascii', 'ignore').decode()
# Remove mentions
x = re.sub("@\S+", " ", x)
# Remove URL
x = re.sub("https*\S+", " ", x)
# Remove Hashtags
x = re.sub("#\S+", " ", x)
# Remove ticks and the next character
x = re.sub("\'\w+", '', x)
# Remove numbers
x = re.sub(r'\w*\d+\w*', '', x)
# Replace the over spaces
x = re.sub('\s{2,}', " ", x)
# # print(x)
# Removing punctuations
my_punct = ['!', '"', '#', '$', '%', '&', "'", '(', ')', '*', '+', ',','.',
           '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\', ']', '^', '_', 
           '`', '{', '|', '}', '~', '»', '«', '“', '”']

punct_pattern = re.compile("[" + re.escape("".join(my_punct)) + "]")
x = re.sub(punct_pattern, "", x)
#removing wiki related stop words
stopwords=['bibcode','doi', 'arxiv', 'isbn', 'jump', 'navigation', 'search']
for i in stopwords:
  n=x.replace(i,'')
  x=n

f = open("text.txt", "w")
f.write(x)
f.close()

"""# B) NLTK Operations"""

# pip install nltk
import nltk
import matplotlib.pyplot as plt

"""## Tokenization(Sentences,Tokens)"""
from nltk.tokenize import word_tokenize
nltk.download('punkt')
tokens = word_tokenize(x)
# print(tokens)

plt.figure(figsize=(15, 5))
fd = nltk.FreqDist(tokens)
fd.plot(50,cumulative=False)

"""## Removing Stopwords"""

nltk.download('stopwords')
from nltk.corpus import stopwords

tokens_without_stopwords = []
sr = stopwords.words('english')

for token in tokens:
    if token not in sr:
        tokens_without_stopwords.append(token)

# print(tokens_without_stopwords)

plt.figure(figsize=(15, 5))
fd = nltk.FreqDist(tokens_without_stopwords)
fd.plot(50,cumulative=False)

"""## POS tagging

"""

nltk.download('averaged_perceptron_tagger')
from nltk import pos_tag
# POS (Parts Of Speech) for: nouns, adjectives, verbs and adverbs
# DI_POS_TYPES = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'} 
# POS_TYPES = list(DI_POS_TYPES.keys())

pos = nltk.pos_tag(tokens_without_stopwords)
# print(pos)

from collections import Counter
counts = Counter( tag for word,  tag in pos)
# print(counts)

import matplotlib.pyplot as plt
plt.figure(figsize=(15, 5))

names = list(counts.keys())
values = list(counts.values())

plt.bar(range(len(counts)), values, tick_label=names)
plt.show()

nouns = []
for (a, b) in pos:
  if (b == "NN"):
    nouns.append(a)

# print(nouns)

plt.figure(figsize=(15, 5))
fd = nltk.FreqDist(nouns)
fd.plot(50,cumulative=False)

Adj = []
for (a, b) in pos:
  if (b == "JJ"):
    Adj.append(a)

# print(Adj)

plt.figure(figsize=(15, 5))
fd = nltk.FreqDist(Adj)
fd.plot(50,cumulative=False)

pnouns = []
for (a, b) in pos:
  if (b == "NNS"):
    pnouns.append(a)

# print(pnouns)

plt.figure(figsize=(15, 5))
fd = nltk.FreqDist(pnouns)
fd.plot(50,cumulative=False)

"""## Stemming and Lemmatization"""

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

# lemmas list of words
lemmas = []

lemmatizer = WordNetLemmatizer()
for word in tokens_without_stopwords:
  lemmas.append(lemmatizer.lemmatize(word))

# print(lemmas)

plt.figure(figsize=(15, 5))
fd = nltk.FreqDist(lemmas)
fd.plot(50,cumulative=False)

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

stems = []
# list of stems
for word in tokens_without_stopwords:
  stems.append(stemmer.stem(word))

# print(stems)

plt.figure(figsize=(15, 5))
fd = nltk.FreqDist(stems)
fd.plot(50,cumulative=False)

"""# C) Word cloud

## The Algorithm

  1. Lemmatize Adjectives
  2. Combine nouns(plural, singular) and Adjectives(lemmatized) [based on POS tagging]
  3. Pick the top 50 common words among this list
"""

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

# lemmas list of words
snouns = []

lemmatizer = WordNetLemmatizer()
for pnoun in pnouns:
  snouns.append(lemmatizer.lemmatize(pnoun))

# print(snouns)

plt.figure(figsize=(15, 5))
fd = nltk.FreqDist(snouns)
fd.plot(50,cumulative=False)

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

# lemmas list of words
adjnouns = []

lemmatizer = WordNetLemmatizer()
for word in Adj:
  adjnouns.append(lemmatizer.lemmatize(word))

# print(adjnouns)

plt.figure(figsize=(15, 5))
fd = nltk.FreqDist(adjnouns)
fd.plot(50,cumulative=False)

from collections import Counter

counts1 = Counter(nouns)
# print(counts1)
counts2 = Counter(pnouns)
# print(counts2)
counts3 = Counter(adjnouns)
# print(counts3)

from collections import Counter

final = []
for noun in nouns:
  final.append(noun)
for word in pnouns:
  final.append(word)
for word in adjnouns:
  final.append(word)

# removing wiki related junk words
stopwords =['a', 'b', 'c', 'd', 'e', 'articles', 'pmid']
for word in list(final):  
  if word in stopwords:
      final.remove(word)

counts = Counter(final)
final = counts.most_common(50)

mylist = []
print("Final List of words according to their importance: ")
for key, value in final:
  print(key)
  mylist.append(key)

output=('\n').join(mylist)

f = open("output.txt", "w")
f.write(output)
f.close()

"""## Word cloud visualisation"""

import matplotlib.pyplot as plt
from wordcloud import WordCloud

#convert list to string and generate
unique_string=(" ").join(mylist)
wordcloud = WordCloud(max_words= 75, width = 1000, height = 1000, background_color ='white').generate(unique_string)
plt.figure(figsize=(15,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.savefig("word cloud"+".png", bbox_inches='tight')
plt.show()
plt.close()